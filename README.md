# sgd-error-bound-logreg
Empirically verifying stochastic gradient descent's (SGD) theoretical error bound for convex functions via logistic regression loss function.  
The theoretical error bound for convex functions can be found in the book by Shalev-Shwartz and Ben-David, *Understanding Machine Learning: From Theory to Algorithms*, Theorem 14.8. This repo contains a jupyter notebook which tests this error bound. To run the notebook, make sure you have numpy, numba, matplotlib, latex, and the following latex packages installed on your machine: amsmath, amssymb, bm, times. It is not a requirement to have latex, the code will still work properly even if you don't have it installed, but you'll have to remove or comment out the parts of the code that require latex and those packages to run.
If you have all the required packages, you can simply run all the cells in the notebook in a sequential manner.  
The error bound is tested by training a binary logistic regression classifier using SGD. The classifier is trained by running the SGD algorithm to optimize the empirical risk of the classifier. The empirical risk of this classifier is convex, allowing us to utilize the previously mentioned theorem. Data is sampled from two multivariate gaussians. 
